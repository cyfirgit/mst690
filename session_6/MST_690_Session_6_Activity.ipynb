{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f0b94bb9c05233b5ba84e627cb1594a9202c500427df254d50d5b55a704defc9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Show that the following is true (remember that variance is the square of the standard deviation):\n",
    "\n",
    "$\\large cov(x, x) = var(x)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$ \\huge cov(x,x) = \\frac { \\sum_{i=0}^{n} (x_{i} - \\overline{x})(x_{i} - \\overline{x})} {n - 1}  = \\frac { \\sum_{i=0}^{n} (x_{i} - \\overline{x})^{2}} {n - 1} $\n",
    "\n",
    "$ \\huge var(x) = S^{2} = (\\sqrt{\\frac {\\sum_{i=0}^{n} (x_{i} - \\overline{x})^{2}}{n - 1}})^{2} = \\frac { \\sum_{i=0}^{n} (x_{i} - \\overline{x})^{2}} {n - 1} $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Explain the benefits of dimensionality reduction on large data sets. In what ways might dimensionality reduction be detrimental? Recall from our Clustering lecture the “Curse of Dimensionality.” How might the utility of dimensionality reduction be explained in  this context?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*In general, by reducing the dimensionality of your data, you are able to run analytical algorithms faster.  There is a tradeoff; reducing dimensions still means losing data, and so it \"blurs out\" the finer complexities of your data.  Viewed through the lens of the \"curse of dimensionality,\" high-dimensional sets become computationally intractable as the scale of metrics like Euclidean distance approach infinity.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Calculate (by hand) the eigenvalues of matrix A. Calculate both the eigenvalues AND the eigenvectors using Python:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"s6q3.jpg\" width=800>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np \n",
    "\n",
    "matrix = np.array([[5,7],[-2,-4]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "print(eigenvalues)\n",
    "print(eigenvectors)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 3. -2.]\n[[ 0.96152395 -0.70710678]\n [-0.27472113  0.70710678]]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "### 4. Compute (by hand) the determinant of matrix B:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"s6q4.jpg\" width=800>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### See Data_Science_Mathematics_Session_6.ipynb for Question 5."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}